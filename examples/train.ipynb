{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "import optax\n",
    "import equinox as eqx \n",
    "from jax import Array, numpy as jnp, nn\n",
    "\n",
    "from qwen import QwenModel, utils, generate, forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, inputs, targets, callback):\n",
    "    optimizer = optax.chain(optax.clip_by_global_norm(1.0), optax.adam(1e-4))\n",
    "    opt_state = optimizer.init(eqx.filter(model, eqx.is_array))\n",
    "\n",
    "    def loss_fn(model, inputs, targets):\n",
    "        logits = forward(model, inputs)\n",
    "        return optax.softmax_cross_entropy_with_integer_labels(logits, targets).mean()\n",
    "\n",
    "    @eqx.filter_jit\n",
    "    def step(model, opt_state, inputs, targets):\n",
    "        loss, grads = eqx.filter_value_and_grad(loss_fn)(model, inputs, targets)\n",
    "        updates, opt_state = optimizer.update(grads, opt_state, model)\n",
    "\n",
    "        model = eqx.apply_updates(model, updates)\n",
    "        return model, opt_state, loss\n",
    "\n",
    "    for batch_inputs, batch_targets in zip(inputs, targets):\n",
    "        model, opt_state, loss = step(model, opt_state, batch_inputs, batch_targets)\n",
    "        callback(model, loss)\n",
    "    return model\n",
    "\n",
    "\n",
    "def predict(model, tokenizer, prompt, max_tokens=30):\n",
    "    inputs = jnp.array(tokenizer(prompt, return_tensors=\"np\").input_ids)\n",
    "    out_ids = generate(model, inputs, max_tokens=max_tokens)\n",
    "    return tokenizer.decode(out_ids[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "def load_dataset(path, tokenizer, batch_size=30, seq_len=30):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        text = f.read().strip()\n",
    "\n",
    "    tokens = tokenizer(text, return_tensors=\"np\").input_ids.squeeze()\n",
    "    num_chunks = len(tokens) // (seq_len + 1)\n",
    "    num_batches = num_chunks // batch_size\n",
    "\n",
    "    tokens = tokens[: num_chunks * (seq_len + 1)].reshape(num_chunks, seq_len + 1)\n",
    "    inputs, targets = tokens[:, :-1], tokens[:, 1:]\n",
    "\n",
    "    inputs = inputs[: num_batches * batch_size].reshape(\n",
    "        num_batches, batch_size, seq_len\n",
    "    )\n",
    "    targets = targets[: num_batches * batch_size].reshape(\n",
    "        num_batches, batch_size, seq_len\n",
    "    )\n",
    "    return jnp.array(inputs), jnp.array(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.71256: \n",
      "Student: What causes lightning?\n",
      "Teacher: Electricity! What do you think?!?\n",
      "Student: Why do dogs bark?\n",
      "Teacher: To let you know they’re hungry!?\n",
      "Student: Why do\n",
      "\n",
      "3.97182: \n",
      "Student: What causes lightning?\n",
      "Teacher: Because of electromagnetic fields, of course, you buffoon, you’re not just some crazy, no, you don’t just have a superhuman brain\n",
      "\n",
      "3.70153: \n",
      "Student: What causes lightning?\n",
      "Teacher: The discharge of a spark! No, it’s not a “flash of lightning”! No, it’s not “electricity”! No,\n",
      "\n",
      "2.96777: \n",
      "Student: What causes lightning?\n",
      "Teacher: Because of rapid discharge of electrical energy! No, it doesn’t ‘‘‘‘ ‘‘ ‘‘ ‘‘ ‘‘ ‘‘ ‘‘ ‘\n",
      "\n",
      "2.34997: \n",
      "Student: What causes lightning?\n",
      "Teacher: Because of lightning strikes! No, it’s not ‘a lightning bolt’!\n",
      "\n",
      "Student: How do submarines sink?\n",
      "Teacher: By using buoyancy tanks\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_name = \"Qwen/Qwen2.5-0.5B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "hf_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, torch_dtype=torch.float32, device_map=\"cpu\"\n",
    ")\n",
    "model = utils.from_hf(hf_model)\n",
    "inputs, targets = load_dataset(\"dataset.txt\", tokenizer)\n",
    "\n",
    "prompt = \"\"\"Student: What causes lightning?\n",
    "Teacher:\"\"\"\n",
    "\n",
    "def eval_callback(model, loss):\n",
    "    print(f\"loss: {loss:.5f}\\n{predict(model, tokenizer, prompt)}\\n\")\n",
    "\n",
    "model = train_model(model, inputs, targets, eval_callback)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qwen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
